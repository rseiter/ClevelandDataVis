---
title: "Likehood Estimates for Outcome Rank Given Correlated Variable"
author: "Rich Seiter"
date: "Sunday, July 27, 2014"
output: html_document
---

Based on idea described in my comment at http://infoproc.blogspot.com/2014/07/success-ability-and-all-that.html  
Also see the [original LessWrong post](http://lesswrong.com/lw/km6/why_the_tails_come_apart/)

Given a contributing factor X and a related outcome variable Y (with correlation r).  Calculate likelihood that a given X will have the largest Y.  An example case might be height as X and basketball ability as Y.  Here we will focus on abstract normal variables with mean 0 and standard deviation 1.  I am assuming a correlation of 0.65, but it will be interesting to vary that.

Ideally this would be computed exactly.  This analysis uses a simulation instead.

First we generate a single set of data to give a sense of what we will see.

```{r example}
Sys.time()
# These variables define our distribution
Xmean <- 0
Ymean <- 0
Xsd <- 1
Ysd <- 1
#XYcor <- 0.65
XYcor <- 0.4
#XYcor <- 0.2
#XYcor <- 0.0

# Derived variables
XYmean <- c(Xmean, Ymean)
XYcov <- matrix(c(1, XYcor, XYcor, 1), nrow=2, ncol=2)

# Create our multivariate normal distribution using mvrnorm in MASS
require(MASS)
 
#make this reproducible
set.seed(2)
 
# Don't make sample too large here, car scatterplot can be slow
examplePopSize <- 1e5
myDrawsExample <- mvrnorm(examplePopSize, mu=XYmean, Sigma=XYcov)
myDrawsExample <- data.frame(myDrawsExample)
colnames(myDrawsExample) <- c("X", "Y")

#check the mean, sd, and cor
apply(myDrawsExample, 2, mean)
apply(myDrawsExample, 2, sd)
cor(myDrawsExample)

# Plot the distribution
require(car)
scatterplot(Y ~ X, data=myDrawsExample, ellipse=TRUE, levels=c(0.5, 0.95, 0.99, 0.999, 0.9999))
```

Simulate Multiple Populations
-----------------------------

```{r simulation}
Sys.time()
# These values take about 15 minutes
# numRuns <- 1e4
# popSize <- 1e5
numRuns <- 1e4
popSize <- 1e5

# Quick version for debugging
# numRuns <- 1e2
# popSize <- 1e4

# Also record X for a top portion of Y
topProp <- 0.001 # Try 0.1%
topN <- popSize*topProp

XOfLargestY <- 1:numRuns
XOfTopPropY <- 1:(numRuns*topN)

#make this reproducible
set.seed(2)
for (i in 1:numRuns) {
  myDraws <- mvrnorm(popSize, mu=XYmean, Sigma=XYcov)
  myDraws <- data.frame(myDraws)
  colnames(myDraws) <- c("X", "Y")
  XOfLargestY[i] <- myDraws$X[which.max(myDraws$Y)]
  XOfTopPropY[((i-1)*topN+1):(i*topN)] <- myDraws$X[head(order(myDraws$Y, decreasing=TRUE), n=topN)]
}
Sys.time()
```

Look at the Results
-------------------

How often does the largest Y have a given X value?

```{r}
require(lattice)
densityplot(XOfLargestY)

# Take a closer look at this distribution
summary(XOfLargestY)
mean(XOfLargestY)
sd(XOfLargestY)
```

How likely is it that any given person at level X will have the best Y?

I'm not sure about these calculations (especially the attempt to normalize to a probability).  Feedback welcomed.

```{r}
obs_dvals <- density(XOfLargestY)
dvals <- dnorm(obs_dvals$x,mean=0,sd=1)
prob <- data.frame(x = obs_dvals$x,
                    y = obs_dvals$y / dvals / popSize # Attempt to normalize this to prob.
                    )

xyplot(y ~ x, prob, type="l")
xyplot(log10(y) ~ x, prob, type="l")
```

I believe the somewhat anomalous results at the extremes are due to sampling error.  For example, when I increased the popSize from 1e4 to 1e5 (numRuns = 1e2) the peak and dip at the right moved right by about 0.7 to correspond.  Then when I increased popSize to 1e6 the peak moved another ~0.3 right.  The center of the XOfLargestY distribution also shifted right and the probabilities for 3-4SD X decreased.

Now Look at the Top `r topProp*100`% of Y
-----------------------------------------

How often does the top `r topProp*100`% of Y have a given X value?

```{r}
densityplot(XOfTopPropY)

# Take a closer look at this distribution
summary(XOfTopPropY)
mean(XOfTopPropY)
sd(XOfTopPropY)
```

How likely is it that any given person at level X will be in the top `r topProp*100`% of Y?

```{r}
obs_dvals1 <- density(XOfTopPropY)
dvals1 <- dnorm(obs_dvals1$x,mean=0,sd=1)
prob1 <- data.frame(x = obs_dvals1$x,
                    y = obs_dvals1$y / dvals1 / popSize * topN # Attempt to normalize this to prob.
                    )

xyplot(y ~ x, prob1, type="l")
xyplot(log10(y) ~ x, prob1, type="l")
```

Plot the densities together.

```{r}
plot(density(XOfLargestY), xlim=c(min(XOfTopPropY), max(XOfTopPropY)), col="red",
#plot(density(XOfLargestY), xlim=c(min(XOfLargestY), max(XOfLargestY)), col="red",
     main=paste("Compare Densities of Largest and Top", topProp*100, "% of Y"),
     xlab="X")
lines(density(XOfTopPropY))
legend("topright", legend=c("Largest", paste("Top", topProp*100, "%")), lty=1, col=c("red", "black"))
```

Note limiting of X axis.  I think this is the valid range for inference.

```{r}
#plot(prob$x, log10(prob$y), xlim=c(min(XOfTopPropY), max(XOfTopPropY)),
plot(prob$x, log10(prob$y), xlim=c(min(XOfLargestY), max(XOfLargestY)),
     ylim=c(min(log10(prob1$y)),max(log10(prob1$y))),
     col="red",
     main=paste("Compare Per-Person Likelihood of Largest and Top", topProp*100, "% of Y"),
     xlab="X", ylab="log10(probability)", type="l")
lines(prob1$x, log10(prob1$y))
legend("topleft", legend=c("Largest", paste("Top", topProp*100, "%")), lty=1, col=c("red", "black"))
```

Comments
--------

Clearly this simulation model has many assumptions.  The two that particularly concern me are:  
1. Normality (e.g. IQ is believed to have fat tails)  
2. That the sample population is large enough.  I saw consistent changes when increasing popSize.  I finally chose 1e5 as a compromise.

I think it best to only really consider the results in the X range where XOfLargestY has at least one value (and possibly ignore apparent extreme outliers).


Berkson's Paradox
-----------------

[This Less Wrong comment](http://lesswrong.com/lw/km6/why_the_tails_come_apart/b5r2) notes the relevance of [Berkson's Paradox](http://en.wikipedia.org/wiki/Berkson's_paradox)

Though not strictly applicable here (X and Y are not independent) it does raise the question of what correlations look like if we look at selected portions of the population.

Use the single example population created above.  Look at > 2SD for X, Y, and both.

Notice how results change for different correlations.  As correlations become small the sample with both >2SD becomes smaller.

Overall it looks like truncating the distributions decreases the measured correlations with the largest effect being if both X and Y are truncated.

```{r}
myDrawsSampleY2SD <- myDrawsExample[myDrawsExample$Y > 2,]

#check the mean, sd, and cor
apply(myDrawsSampleY2SD, 2, mean)
apply(myDrawsSampleY2SD, 2, sd)
cor(myDrawsSampleY2SD)

# Plot the distribution
require(car)
scatterplot(Y ~ X, data=myDrawsSampleY2SD, ellipse=TRUE, levels=c(0.5, 0.95))
title("Sample with Y > 2SD")
```

```{r}
myDrawsSampleX2SD <- myDrawsExample[myDrawsExample$X > 2,]

#check the mean, sd, and cor
apply(myDrawsSampleX2SD, 2, mean)
apply(myDrawsSampleX2SD, 2, sd)
cor(myDrawsSampleX2SD)

# Plot the distribution
require(car)
scatterplot(Y ~ X, data=myDrawsSampleX2SD, ellipse=TRUE, levels=c(0.5, 0.95))
title("Sample with X > 2SD")
```

```{r}
myDrawsSampleXY2SD <- myDrawsExample[(myDrawsExample$X > 2) & (myDrawsExample$Y > 2),]

#check the mean, sd, and cor
apply(myDrawsSampleXY2SD, 2, mean)
apply(myDrawsSampleXY2SD, 2, sd)
cor(myDrawsSampleXY2SD)

# Plot the distribution
require(car)
scatterplot(Y ~ X, data=myDrawsSampleXY2SD, ellipse=TRUE, levels=c(0.5, 0.95))
title("Sample with both X and Y > 2SD")
```


Other Notes
-----------

A relevant point by another commenter: [True or False: Half of All 7-Footers are in the NBA](http://www.truthaboutit.net/2012/05/true-or-false-half-of-all-7-footers-are-in-the-nba.html)